{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RksSRkreIvPg",
        "outputId": "c9829ca1-1d65-440a-a643-d5c14a198c1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model_name2 = \"openai-community/gpt2\"\n",
        "\n",
        "# Create a text-generation pipeline using the loaded model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "summarizer_pipeline = pipeline(\n",
        "    \"summarization\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    framework=\"pt\"\n",
        ")\n",
        "\n",
        "model2 = AutoModelForCausalLM.from_pretrained(model_name2)\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "\n",
        "# Create a text-generation pipeline using the loaded model\n",
        "aggregator_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model= model2,\n",
        "    tokenizer=tokenizer2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bOtdciDJhWtf"
      },
      "outputs": [],
      "source": [
        "def generate_summary(text, max_length=130, min_length=30, temperature=1.0):\n",
        "    # Clean and prepare input text\n",
        "    joined_responses = \"\\n\".join(text)\n",
        "    cleaned_text = joined_responses.strip()\n",
        "\n",
        "    new_max_length = min(max_length, len(cleaned_text))\n",
        "    if len(cleaned_text) < max_length:\n",
        "      new_max_length = max(len(cleaned_text) - 10, min_length + 5)\n",
        "\n",
        "    # Generate summary\n",
        "    summary = summarizer_pipeline(\n",
        "        cleaned_text,\n",
        "        max_length=new_max_length,\n",
        "        min_length=min_length,\n",
        "        temperature=temperature,\n",
        "        do_sample=False\n",
        "    )\n",
        "    print(summary)\n",
        "    return summary[0]['summary_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "51sHGOtGh4hO"
      },
      "outputs": [],
      "source": [
        "def aggregate_response(user_prompt: str, summarized_text: str, max_length: int = 130, min_length: int=30, temperature: float = 0.7) -> str:\n",
        "  # Construct the aggregator prompt\n",
        "    aggregator_prompt = f\"\"\"\n",
        "You are given the user's prompt, along with a summarization of expert responses to this prompt. Your task is to create a\n",
        "helpful response to the user's prompt basedon the summary of the expert responses.\n",
        "\n",
        "\n",
        "User Query:\n",
        "{user_prompt}\n",
        "\n",
        "Summarization of Expert Responses:\n",
        "{summarized_text}\n",
        "\n",
        "[ANSWER_START]\n",
        "\"\"\"\n",
        "\n",
        "    # Generate the aggregated answer\n",
        "    generated_output = aggregator_pipeline(\n",
        "        aggregator_prompt,\n",
        "        max_length=len(aggregator_prompt) + max_length,\n",
        "        min_length=min_length,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer2.eos_token_id,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Extract the generated text\n",
        "    aggregated_answer = generated_output[0]['generated_text']\n",
        "    if \"[ANSWER_START]\" in aggregated_answer:\n",
        "        aggregated_answer = aggregated_answer.split(\"[ANSWER_START]\")[1].strip()\n",
        "    else:\n",
        "        aggregated_answer = aggregated_answer[len(aggregator_prompt):].strip()\n",
        "\n",
        "    return aggregated_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBJWmOyy60DE",
        "outputId": "bfcdd7ed-97b7-48f3-b258-602d35534291"
      },
      "outputs": [],
      "source": [
        "def final_answer(user_prompt: str, expert_outputs: list[str], max_length: int = 130, min_length: int=30, temperature: float = 1.0) -> str:\n",
        "    \"\"\"\n",
        "    Aggregates the responses from multiple expert models into a coherent answer using a Llama-based model.\n",
        "\n",
        "    Args:\n",
        "        user_prompt (str): The original user prompt.\n",
        "        expert_outputs (list[str]): A list of responses from expert models.\n",
        "        max_length (int): The maximum length of the generated output.\n",
        "        temperature (float): Sampling temperature for generation.\n",
        "\n",
        "    Returns:\n",
        "        str: A coherent aggregated answer.\n",
        "    \"\"\"\n",
        "    # get summary text\n",
        "    summarized_text = generate_summary(expert_outputs, max_length, min_length, temperature)\n",
        "\n",
        "    aggregated_answer = aggregate_response(user_prompt, summarized_text, max_length, min_length, 0.7)\n",
        "\n",
        "    return aggregated_answer\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    user_query = \"How can I optimize the performance of my deep learning models in production?\"\n",
        "    expert_responses = [\n",
        "        \"Optimizing data pipelines and leveraging hardware accelerators like GPUs or TPUs will further improve throughput.\",\n",
        "        \"Consider using model quantization and pruning techniques to reduce model size and inference latency.\",\n",
        "        \"Implementing efficient serving architectures such as TensorRT or ONNX Runtime can boost performance.\"\n",
        "    ]\n",
        "\n",
        "    final_answer = final_answer(user_query, expert_responses)\n",
        "    print(\"Aggregated Answer:\\n\", final_answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
