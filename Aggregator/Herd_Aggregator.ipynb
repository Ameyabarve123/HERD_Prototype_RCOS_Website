{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBJWmOyy60DE",
        "outputId": "fd0e23c2-5fab-42ed-d88a-9c537add4946"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aggregated Answer:\n",
            " New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_name = \"facebook/bart-large-cnn\"  # LLama LLM model I am using for the aggregator\n",
        "model_name2 = \"openai-community/gpt2\"\n",
        "\n",
        "# Create a text-generation pipeline using the loaded model\n",
        "summarizer_pipeline = pipeline(\n",
        "    \"summarization\",\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name2)  \n",
        "model = AutoModelForCausalLM.from_pretrained(model_name2)  \n",
        "\n",
        "# Create a text-generation pipeline using the loaded model\n",
        "aggregator_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"openai-community/gpt2\",\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "def aggregate_response(user_prompt: str, expert_outputs: list[str], max_length: int = 150, temperature: float = 0.7) -> str:\n",
        "    \"\"\"\n",
        "    Aggregates the responses from multiple expert models into a coherent answer using a Llama-based model.\n",
        "\n",
        "    Args:\n",
        "        user_prompt (str): The original user prompt.\n",
        "        expert_outputs (list[str]): A list of responses from expert models.\n",
        "        max_length (int): The maximum length of the generated output.\n",
        "        temperature (float): Sampling temperature for generation.\n",
        "\n",
        "    Returns:\n",
        "        str: A coherent aggregated answer.\n",
        "    \"\"\"\n",
        "    # Combine expert responses into a single context string.\n",
        "    expert_responses = \"\\n\".join(\n",
        "        f\"Expert {i+1}: {response}\"\n",
        "        for i, response in enumerate(expert_outputs)\n",
        "    )\n",
        "    # summarize prompts\n",
        "    summarization_prompt = f\"\"\"\n",
        "Below are responses from multiple experts. Create at most one paragraph, comprehensive summary, that combines their key points.\n",
        "\n",
        "Expert Responses:\n",
        "{expert_responses}\n",
        "\n",
        "[SUMMARY_START]:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    summarized_text_model = summarizer_pipeline(\n",
        "        summarization_prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        do_sample=True \n",
        "    )\n",
        "\n",
        "    summarized_text = summarized_text_model[0]['generated_text']\n",
        "\n",
        "    if \"[SUMMARY_START]\" in summarized_text:\n",
        "        summarized_text = summarized_text.split(\"[SUMMARY_START]\")[1].strip()\n",
        "    else:\n",
        "      summarized_text = summarized_text[len(summarization_prompt):].strip()\n",
        "    # Construct the aggregator prompt.\n",
        "    aggregator_prompt = f\"\"\"\n",
        "You are designed to analyze the given user query, and the summarization of responses of experts to the same query. Using this,\n",
        "make sure you produce a meaningful and helpful response. Synthesize the information to produce a single, clear aggregated answer.\n",
        "\n",
        "User Query:\n",
        "{user_prompt}\n",
        "\n",
        "Summarization of Expert Responses:\n",
        "{summarized_text}\n",
        "\n",
        "[ANSWER_START]\n",
        "\"\"\"\n",
        "    print(summarized_text)\n",
        "    # Generate the aggregated answer using the Llama-based aggregator model.\n",
        "    generated_output = aggregator_pipeline(\n",
        "        aggregator_prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        do_sample=True  # Set to False for deterministic output\n",
        "    )\n",
        "\n",
        "    # The output is a list of dictionaries; extract the generated text.\n",
        "    aggregated_answer = generated_output[0]['generated_text']\n",
        "    if \"[ANSWER_START]\" in aggregated_answer:\n",
        "        aggregated_answer = aggregated_answer.split(\"[ANSWER_START]\")[1].strip()\n",
        "    else:\n",
        "        # Fallback: take only the new generated text\n",
        "        aggregated_answer = aggregated_answer[len(aggregator_prompt):].strip()\n",
        "\n",
        "    return aggregated_answer\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Original user prompt\n",
        "    user_query = \"How can I optimize the performance of my deep learning models in production?\"\n",
        "\n",
        "    # Sample responses from expert models. The router should input these responses into the aggregator at a later step\n",
        "    expert_responses = [\n",
        "        \"Consider using model quantization and pruning techniques to reduce model size and inference latency.\",\n",
        "        \"Implementing efficient serving architectures such as TensorRT or ONNX Runtime can boost performance.\",\n",
        "        \"Optimizing data pipelines and leveraging hardware accelerators like GPUs or TPUs will further improve throughput.\"\n",
        "    ]\n",
        "\n",
        "    final_answer = aggregate_response(user_query, expert_responses)\n",
        "    print(\"Aggregated Answer:\\n\", final_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4NpiUzW7KHz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
