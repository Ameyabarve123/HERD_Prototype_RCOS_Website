{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.3-70B-Instruct\"  # LLama LLM model I am using for the aggregator\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)  # converts raw text into tokens that the model can understand\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")  # transformer-based model specifically designed for casual language modeling\n",
        "\n",
        "# Create a text-generation pipeline using the loaded model\n",
        "aggregator_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=model.device if hasattr(model, \"device\") else 0\n",
        ")\n",
        "\n",
        "def aggregate_response(user_prompt: str, expert_outputs: list[str], max_length: int = 512, temperature: float = 1e-10) -> str:\n",
        "    \"\"\"\n",
        "    Aggregates the responses from multiple expert models into a coherent answer using a Llama-based model.\n",
        "\n",
        "    Args:\n",
        "        user_prompt (str): The original user prompt.\n",
        "        expert_outputs (list[str]): A list of responses from expert models.\n",
        "        max_length (int): The maximum length of the generated output.\n",
        "        temperature (float): Sampling temperature for generation.\n",
        "\n",
        "    Returns:\n",
        "        str: A coherent aggregated answer.\n",
        "    \"\"\"\n",
        "    # Combine expert responses into a single context string.\n",
        "    combined_expert_context = \"\\n\\n\".join(\n",
        "        [f\"Expert {i+1}:\\n{output}\" for i, output in enumerate(expert_outputs)]\n",
        "    )\n",
        "\n",
        "    # Construct the aggregator prompt.\n",
        "    aggregator_prompt = f\"\"\"\n",
        "You are an aggregator LLM designed to combine multiple expert responses into one coherent, concise answer.\n",
        "Below is the original user query and responses from several experts. Synthesize the information to produce a single, clear aggregated answer.\n",
        "\n",
        "User Query:\n",
        "{user_prompt}\n",
        "\n",
        "Expert Responses:\n",
        "{combined_expert_context}\n",
        "\n",
        "Provide a clear and concise aggregated answer:\n",
        "\"\"\"\n",
        "    # Generate the aggregated answer using the Llama-based aggregator model.\n",
        "    generated_output = aggregator_pipeline(\n",
        "        aggregator_prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        do_sample=True  # Set to False for deterministic output\n",
        "    )\n",
        "\n",
        "    # The output is a list of dictionaries; extract the generated text.\n",
        "    aggregated_answer = generated_output[0]['generated_text']\n",
        "\n",
        "    # triming the prompt from the aggregated answer\n",
        "    if aggregator_prompt.strip() in aggregated_answer:\n",
        "        aggregated_answer = aggregated_answer.split(aggregator_prompt.strip())[-1].strip()\n",
        "\n",
        "    return aggregated_answer\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Original user prompt\n",
        "    user_query = \"How can I optimize the performance of my deep learning models in production?\"\n",
        "\n",
        "    # Sample responses from expert models. The router should input these responses into the aggregator at a later step\n",
        "    expert_responses = [\n",
        "        \"Consider using model quantization and pruning techniques to reduce model size and inference latency.\",\n",
        "        \"Implementing efficient serving architectures such as TensorRT or ONNX Runtime can boost performance.\",\n",
        "        \"Optimizing data pipelines and leveraging hardware accelerators like GPUs or TPUs will further improve throughput.\"\n",
        "    ]\n",
        "\n",
        "    # Get the aggregated answer using the Llama model.\n",
        "    final_answer = aggregate_response(user_query, expert_responses)\n",
        "    print(\"Aggregated Answer:\\n\", final_answer)\n"
      ],
      "metadata": {
        "id": "BBJWmOyy60DE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}