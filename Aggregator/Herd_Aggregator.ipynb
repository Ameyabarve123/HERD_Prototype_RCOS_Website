{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_name = \"openai-community/gpt2\"  # LLama LLM model I am using for the aggregator\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)  # converts raw text into tokens that the model can understand\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)  # transformer-based model specifically designed for casual language modeling\n",
        "\n",
        "# Create a text-generation pipeline using the loaded model\n",
        "aggregator_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "def aggregate_response(user_prompt: str, expert_outputs: list[str], max_length: int = 512, temperature: float = 1e-10) -> str:\n",
        "    \"\"\"\n",
        "    Aggregates the responses from multiple expert models into a coherent answer using a Llama-based model.\n",
        "\n",
        "    Args:\n",
        "        user_prompt (str): The original user prompt.\n",
        "        expert_outputs (list[str]): A list of responses from expert models.\n",
        "        max_length (int): The maximum length of the generated output.\n",
        "        temperature (float): Sampling temperature for generation.\n",
        "\n",
        "    Returns:\n",
        "        str: A coherent aggregated answer.\n",
        "    \"\"\"\n",
        "    # Combine expert responses into a single context string.\n",
        "    combined_expert_context = \"\"\n",
        "    for i in range(len(expert_outputs)):\n",
        "      combined_expert_context += (expert_outputs[i] + \"\\nNEWPROMPT\\n\")\n",
        "\n",
        "    # summarize prompts\n",
        "    summarization_prompt = f\"\"\"\n",
        "You are an expert text summarization LLM that condenses long text into a well-structured, detailed summary, while preserving key insights.\n",
        "Each prompt is seperated by an indentation, then a line that just says 'NEWPROMPT', and then another indentation.\n",
        "Analyze the given text, extract the most important information, and structure them in a clear and logical manner.\n",
        "\n",
        "Text to Summarize:\n",
        "{combined_expert_context}\n",
        "\n",
        "Provide a clear and concise summary of the given text:\n",
        "\"\"\"\n",
        "\n",
        "    summarized_text_model = aggregator_pipeline(\n",
        "        summarization_prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        do_sample=True  # Set to False for deterministic output\n",
        "    )\n",
        "\n",
        "    summarized_text = summarized_text_model[0]['generated_text']\n",
        "\n",
        "    if summarization_prompt.strip() in summarized_text:\n",
        "        summarized_text = summarized_text.split(summarization_prompt.strip())[-1].strip()\n",
        "\n",
        "    # Construct the aggregator prompt.\n",
        "    aggregator_prompt = f\"\"\"\n",
        "You are an aggregator LLM designed to analyze the given user query, and the summarization of responses of experts to the same query. The inputs you\n",
        "are given, are intended to make sure you produce a meaningful and helpful response.Below is the original user query and responses from several experts.\n",
        "Synthesize the information to produce a single, clear aggregated answer.\n",
        "\n",
        "User Query:\n",
        "{user_prompt}\n",
        "\n",
        "Summarization of Expert Responses:\n",
        "{summarized_text}\n",
        "\n",
        "Provide a clear and concise aggregated answer:\n",
        "\"\"\"\n",
        "    # Generate the aggregated answer using the Llama-based aggregator model.\n",
        "    generated_output = aggregator_pipeline(\n",
        "        aggregator_prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        do_sample=True  # Set to False for deterministic output\n",
        "    )\n",
        "\n",
        "    # The output is a list of dictionaries; extract the generated text.\n",
        "    aggregated_answer = generated_output[0]['generated_text']\n",
        "\n",
        "    # triming the prompt from the aggregated answer\n",
        "    if aggregator_prompt.strip() in aggregated_answer:\n",
        "        aggregated_answer = aggregated_answer.split(aggregator_prompt.strip())[-1].strip()\n",
        "\n",
        "    return aggregated_answer\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Original user prompt\n",
        "    user_query = \"How can I optimize the performance of my deep learning models in production?\"\n",
        "\n",
        "    # Sample responses from expert models. The router should input these responses into the aggregator at a later step\n",
        "    expert_responses = [\n",
        "        \"Consider using model quantization and pruning techniques to reduce model size and inference latency.\",\n",
        "        \"Implementing efficient serving architectures such as TensorRT or ONNX Runtime can boost performance.\",\n",
        "        \"Optimizing data pipelines and leveraging hardware accelerators like GPUs or TPUs will further improve throughput.\"\n",
        "    ]\n",
        "\n",
        "    # Get the aggregated answer using the Llama model.\n",
        "    final_answer = aggregate_response(user_query, expert_responses)\n",
        "    print(\"Aggregated Answer:\\n\", final_answer)\n"
      ],
      "metadata": {
        "id": "BBJWmOyy60DE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0e23c2-5fab-42ed-d88a-9c537add4946"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated Answer:\n",
            " New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New PROMPT\n",
            "\n",
            "New\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4NpiUzW7KHz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}